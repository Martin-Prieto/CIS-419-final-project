{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20dc07d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchaudio in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.10.0)\n",
      "Requirement already satisfied: torch==1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchaudio) (1.10.0)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch==1.10.0->torchaudio) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch==1.10.0->torchaudio) (3.10.0.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for six: [Errno 2] No such file or directory: '/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/six-1.16.0.dist-info/METADATA'\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: librosa in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.8.1)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (3.3.4)\n",
      "Requirement already satisfied: tensorflow_io in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.21.0)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (2.1.9)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (1.0.1)\n",
      "Requirement already satisfied: numba>=0.43.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (0.53.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (1.5.3)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (0.2.2)\n",
      "Requirement already satisfied: pooch>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (5.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (1.19.5)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (0.10.3.post1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (21.2)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from librosa) (0.24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib) (8.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: tensorflow<2.7.0,>=2.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow_io) (2.6.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow_io) (0.21.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from numba>=0.43.0->librosa) (58.5.3)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from numba>=0.43.0->librosa) (0.36.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pooch>=1.0->librosa) (2.26.0)\n",
      "Requirement already satisfied: appdirs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pooch>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (2.1.0)\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/six-1.16.0.dist-info/METADATA'\n",
      "\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchaudio\n",
    "!pip install librosa matplotlib tensorflow_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "800bb962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchaudio/_internal/module_utils.py:87: UserWarning: Failed to import soundfile. 'soundfile' backend is not available.\n",
      "  warnings.warn(\"Failed to import soundfile. 'soundfile' backend is not available.\")\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "from typing import Tuple\n",
    "import wave\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "import torchaudio.transforms as T\n",
    "from IPython.display import Audio\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import math, random\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from IPython.display import Audio\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c6dac4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[tensor(-0.0644), tensor(-0.0727), tensor(-0....</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[tensor(-0.7592), tensor(-0.8084), tensor(-0....</td>\n",
       "      <td>95.076299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[tensor(-0.0686), tensor(-0.0764), tensor(-0....</td>\n",
       "      <td>70.015840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[tensor(-1.0000), tensor(-1.0000), tensor(-1....</td>\n",
       "      <td>67.107005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[tensor(-0.0646), tensor(-0.0725), tensor(-0....</td>\n",
       "      <td>4.039027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9730</th>\n",
       "      <td>[[tensor(0.2384), tensor(0.2235), tensor(0.209...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9731</th>\n",
       "      <td>[[tensor(0.5547), tensor(0.5233), tensor(0.493...</td>\n",
       "      <td>18.190045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9732</th>\n",
       "      <td>[[tensor(0.2384), tensor(0.2235), tensor(0.209...</td>\n",
       "      <td>0.611846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9733</th>\n",
       "      <td>[[tensor(0.5199), tensor(0.4874), tensor(0.456...</td>\n",
       "      <td>2.546859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9734</th>\n",
       "      <td>[[tensor(0.4746), tensor(0.4464), tensor(0.418...</td>\n",
       "      <td>2.461955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9735 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  audio  intensity\n",
       "0     [[tensor(-0.0644), tensor(-0.0727), tensor(-0....   0.000000\n",
       "1     [[tensor(-0.7592), tensor(-0.8084), tensor(-0....  95.076299\n",
       "2     [[tensor(-0.0686), tensor(-0.0764), tensor(-0....  70.015840\n",
       "3     [[tensor(-1.0000), tensor(-1.0000), tensor(-1....  67.107005\n",
       "4     [[tensor(-0.0646), tensor(-0.0725), tensor(-0....   4.039027\n",
       "...                                                 ...        ...\n",
       "9730  [[tensor(0.2384), tensor(0.2235), tensor(0.209...   0.000000\n",
       "9731  [[tensor(0.5547), tensor(0.5233), tensor(0.493...  18.190045\n",
       "9732  [[tensor(0.2384), tensor(0.2235), tensor(0.209...   0.611846\n",
       "9733  [[tensor(0.5199), tensor(0.4874), tensor(0.456...   2.546859\n",
       "9734  [[tensor(0.4746), tensor(0.4464), tensor(0.418...   2.461955\n",
       "\n",
       "[9735 rows x 2 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"df_final.pkl\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b9b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c9627d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 192000])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rate = 48000\n",
    "df.loc[0, \"audio\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "de7f8a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [-1, 20, 40, 60, 80, 100]\n",
    "labels = [5,4,3,2,1]\n",
    "df['rating'] = pd.cut(df['intensity'], bins,labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "117e9f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>intensity</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[tensor(-0.0644), tensor(-0.0727), tensor(-0....</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[tensor(-0.7592), tensor(-0.8084), tensor(-0....</td>\n",
       "      <td>95.076299</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[tensor(-0.0686), tensor(-0.0764), tensor(-0....</td>\n",
       "      <td>70.015840</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[tensor(-1.0000), tensor(-1.0000), tensor(-1....</td>\n",
       "      <td>67.107005</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[tensor(-0.0646), tensor(-0.0725), tensor(-0....</td>\n",
       "      <td>4.039027</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               audio  intensity rating\n",
       "0  [[tensor(-0.0644), tensor(-0.0727), tensor(-0....   0.000000      5\n",
       "1  [[tensor(-0.7592), tensor(-0.8084), tensor(-0....  95.076299      1\n",
       "2  [[tensor(-0.0686), tensor(-0.0764), tensor(-0....  70.015840      2\n",
       "3  [[tensor(-1.0000), tensor(-1.0000), tensor(-1....  67.107005      2\n",
       "4  [[tensor(-0.0646), tensor(-0.0725), tensor(-0....   4.039027      5"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d1865ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toStereo(aud):\n",
    "    if (aud.shape[0] == 2):\n",
    "        return aud\n",
    "    resig = torch.cat([aud, aud])\n",
    "\n",
    "    return resig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9f8f07c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rezise(aud):\n",
    "    num_rows, sig_len = aud.shape\n",
    "    max_len = 48000//1000 * 4000\n",
    "\n",
    "    if (sig_len > max_len):\n",
    "        aud = aud[:,:max_len]\n",
    "\n",
    "    elif (sig_len < max_len):\n",
    "        pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "        pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "        pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "        pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "        aud = torch.cat((pad_begin, aud, pad_end), 1)\n",
    "      \n",
    "    return aud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59894099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f9d9f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformedData(Dataset):\n",
    "    \"\"\"Transform the first channel of audio and return it together with the rating.\"\"\"\n",
    "    \n",
    "    def __init__(self, df, transforms: nn.Sequential):\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, n: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        example = self.df.loc[n]\n",
    "        channel = example[\"audio\"]\n",
    "        channel_str = toStereo(channel)\n",
    "        channel_rezise = rezise(channel_str)\n",
    "        return self.transforms(channel_rezise), int(example[\"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "94054a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_sample_rate = 16000\n",
    "window_size = 2048\n",
    "hop_size = 256\n",
    "num_mels = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "de1dcfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = nn.Sequential(\n",
    "    T.Resample(orig_freq=sample_rate, new_freq=reduced_sample_rate),\n",
    "    T.MelSpectrogram(\n",
    "        n_mels=num_mels,\n",
    "        n_fft=window_size,\n",
    "        win_length=window_size,\n",
    "        hop_length=hop_size,\n",
    "        power=1,\n",
    "        center=False,\n",
    "        sample_rate=reduced_sample_rate,\n",
    "        f_min=0,\n",
    "        f_max=reduced_sample_rate / 2,\n",
    "        window_fn=torch.hann_window\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "05acc417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 21.4689,  18.8361,  29.7160,  ...,  41.7658,  48.4239,  47.4438],\n",
       "          [ 19.4787,  22.3880,  26.1348,  ..., 120.7119, 123.5909, 118.5946],\n",
       "          [ 16.3506,   8.8245,  11.7256,  ..., 202.9790, 181.3675, 169.8651],\n",
       "          ...,\n",
       "          [  3.6043,   3.1011,   2.9115,  ...,   1.7462,   2.0075,   2.5565],\n",
       "          [  4.0006,   3.6196,   3.5373,  ...,   1.4426,   1.5033,   1.8783],\n",
       "          [  3.3384,   2.9602,   2.7348,  ...,   1.4244,   1.7925,   2.2975]],\n",
       " \n",
       "         [[ 18.3951,  12.1004,  12.5335,  ...,  21.2163,  29.3659,  33.0858],\n",
       "          [ 28.3296,  26.9679,  23.2514,  ...,  15.9717,  20.3360,  37.7313],\n",
       "          [ 15.4397,  15.5327,  16.5888,  ..., 136.4331, 146.8740, 141.9935],\n",
       "          ...,\n",
       "          [  4.4524,   3.9819,   3.7549,  ...,   2.0857,   2.0925,   2.2385],\n",
       "          [  5.0306,   4.3849,   4.0496,  ...,   2.1031,   2.1256,   2.3107],\n",
       "          [  6.6391,   6.3637,   5.0606,  ...,   2.3279,   2.8345,   3.1140]]]),\n",
       " 2)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "myds = TransformedData(df, transforms)\n",
    "\n",
    "# Random split of 80:20 between training and validation\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
    "\n",
    "# Create training and validation data loaders\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "myds[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "836e62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "# ----------------------------\n",
    "# Audio Classification Model\n",
    "# ----------------------------\n",
    "class CNN1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=64, out_features=6)\n",
    "\n",
    "        self.conv1.bias.data.zero_()\n",
    "        self.conv2.bias.data.zero_()\n",
    "        self.conv3.bias.data.zero_()\n",
    "        self.conv4.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Complete the forward pass\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7f40cf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 363.2806\n",
      "epoch: 2, loss: 331.9980\n",
      "epoch: 3, loss: 325.3421\n",
      "epoch: 4, loss: 322.5406\n",
      "epoch: 5, loss: 319.8052\n",
      "epoch: 6, loss: 317.9562\n",
      "epoch: 7, loss: 315.5428\n",
      "epoch: 8, loss: 314.2644\n",
      "epoch: 9, loss: 311.3648\n",
      "epoch: 10, loss: 312.0581\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "model = CNN1()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) \n",
    "train_loss = []\n",
    "# traing iteration\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for itr, (audio, label) in enumerate(train_dl):\n",
    "        # zero gradient\n",
    "        optimizer.zero_grad()\n",
    "        # forward path\n",
    "        y_predicted = model(audio)\n",
    "        loss = criterion(y_predicted, label)\n",
    "        running_loss += loss.item()\n",
    "        # backpropagating\n",
    "        loss.backward()\n",
    "        # optimizes the weights\n",
    "        optimizer.step()\n",
    "    train_loss.append(running_loss)\n",
    "    print(f'epoch: {epoch+1}, loss: {running_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0420ff4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Neural Network is 0.4551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.4328661999857933, pvalue=8.792357338765065e-90)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "correct = 0\n",
    "total = 0\n",
    "array_pred = torch.empty(1)\n",
    "array_correct = torch.empty(1)\n",
    "with torch.no_grad():\n",
    "    for itr, (audio, label) in enumerate(val_dl):\n",
    "        outputs = model(audio)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += predicted.eq(label.reshape(len(label),)).sum() \n",
    "        total += float(len(label))\n",
    "        array_pred = torch.cat((array_pred, predicted), 0)\n",
    "        array_correct = torch.cat((array_correct, label.reshape(len(label),)))\n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy of Neural Network is {accuracy:.4f}')\n",
    "stats.spearmanr(array_correct, array_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ad43e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5387b0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
